{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "import jax.random as jr\n",
    "from jaxtyping import Float, Array, Key\n",
    "import diffrax\n",
    "import equinox as eqx\n",
    "import pandas as pd\n",
    "import optax\n",
    "\n",
    "from models.AutoregressiveCDE import AutoregressiveCDE\n",
    "from utils import train_utils, integrated_gradients, plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is notebook is supposed to check whether Integrated Gradients is correctly implemented together with standardization.\n",
    "More specifically, in the Integrated Gradients paper it is mentioned that a correct implementation will always fulfill\n",
    "\n",
    "\\sum_{i=1}^{n} IG_i(x) = F(x) - F(x')\n",
    "\n",
    "where IG_i is the integraded gradient at the i-th input, x is the original input vector and x' is the baseline vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = eqx.tree_deserialise_leaves(\"serialised_models/autoregressive_cde_14_day_ahead.eqx\", like=AutoregressiveCDE(data_size=3, hidden_size=3, width_size=64, depth=3, key=jr.key(5678)))\n",
    "jax.config.update('jax_enable_x64', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'data/clipped_data.csv'\n",
    "df = pd.read_csv(fname)\n",
    "real_world_ys = jnp.array(df['new_cases'].values) \n",
    "real_world_ys = jnp.log10(1+real_world_ys)\n",
    "ts = jnp.linspace(0,1,100)\n",
    "\n",
    "def integrated_gradients_all_outputs(model: callable,\n",
    "                                     control_until, \n",
    "                                     predict_until,\n",
    "                                     ys: jnp.ndarray,\n",
    "                                     baseline: jnp.ndarray,\n",
    "                                     steps: int,\n",
    "                                     standardize_baseline: bool):\n",
    "\n",
    "    ts = jnp.linspace(0, 1, 100)\n",
    "\n",
    "    ys, mean, std = train_utils.standardize(ys)\n",
    "\n",
    "    if(standardize_baseline):\n",
    "        baseline = (baseline - mean)/std\n",
    "\n",
    "    alphas = jnp.linspace(0.0, 1.0, steps)\n",
    "    interpolated_inputs = baseline + alphas[:, None] * (ys - baseline)\n",
    "\n",
    "    def wrapper(input):\n",
    "        \"\"\"\n",
    "            To preserve Completeness we must treat the inverse log transforms as part of the function being differentiated\n",
    "        \"\"\"\n",
    "        return train_utils.inverse_log_transform(model(ts, ys=input[:control_until], control_until=control_until, saveat=ts, train_until=predict_until) * std + mean)\n",
    "    jac_fn = jax.jacrev(wrapper)     \n",
    "    jacobian_path = jax.vmap(jac_fn)(interpolated_inputs)   \n",
    "    avg_jacobian = jacobian_path.mean(axis=0)              \n",
    "\n",
    "\n",
    "    ig = avg_jacobian * (ys - baseline)[None, :]\n",
    "    \n",
    "    return ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-43.87084203 -46.91622536 -31.6649515   -8.99878639  29.54805714\n",
      "  85.86941838 160.34558679 159.16836789 237.79821607 199.13648506\n",
      " 167.8756028  141.96125513 120.05665005 101.25275285  84.90866145\n",
      "  70.55855983  57.85490565  46.53237693  36.38423892  27.24642141\n",
      "  18.98653722  11.49615447   4.68526205  -1.52175701   0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.        ]\n",
      "[-43.86462251 -46.90827008 -31.66236624  -8.99730648  29.54889759\n",
      "  85.86086675 160.30855316 159.13177355 237.71787093 199.07867702\n",
      " 167.8302377  141.92277411 120.02190609 101.21994014  84.87674342\n",
      "  70.52694919  57.82327882  46.50056348  36.35215439  27.21402615\n",
      "  18.9538113   11.46308264   4.65182521  -1.55558618   0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.        ]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Verify Completeness Axiom for non-standardized baseline\n",
    "\"\"\"\n",
    "baseline = jnp.zeros(100)\n",
    "ig = integrated_gradients_all_outputs(model, 10, 24, real_world_ys[:10], baseline[:10], 400, False)\n",
    "\n",
    "\n",
    "ys_std, mean, std = train_utils.standardize(real_world_ys[:10])\n",
    "\n",
    "\n",
    "f = model(ts, ys=ys_std[:10], control_until=10, saveat=ts, train_until=24)\n",
    "f = train_utils.inverse_log_transform(train_utils.destandardize(f, mean, std))\n",
    "\n",
    "f_bar = model(ts, ys=baseline[:10], control_until=10, saveat=ts, train_until=24)\n",
    "f_bar = train_utils.inverse_log_transform(train_utils.destandardize(f_bar, mean, std))\n",
    "\n",
    "\n",
    "ig_sum = jnp.sum(ig, axis=-1)\n",
    "output_difference = f - f_bar\n",
    "\n",
    "print(ig_sum)\n",
    "print(output_difference)\n",
    "\n",
    "assert(jnp.allclose(output_difference[:10], ig_sum[:10], atol=10e-2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 33.01436421  30.04210934  45.37130874  68.12645263 106.77729541\n",
      " 163.21062442 237.79340792 236.68968063 315.40692365 276.81528784\n",
      " 245.69542818 219.98724554 198.34848705 179.86477962 163.88989895\n",
      " 149.9526604  137.70006558 126.86122007 117.22367807 108.61750235\n",
      " 100.9042689   93.96932837  87.71626365  82.06285823   0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.        ]\n",
      "[ 33.          30.02962812  45.34892965  68.08750845 106.70735228\n",
      " 163.09308132 237.61464711 236.5118658  315.17207984 276.60712036\n",
      " 245.50177984 219.80147014 198.16691795 179.68543836 163.71179456\n",
      " 149.7753893  137.52357328 126.68565425 117.04929557 108.44460925\n",
      " 100.73318164  93.80034888  87.54966385  81.89887108   0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.        ]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Verify Completeness Axiom for standardized baseline. \n",
    "\n",
    "This passes only with more integration steps and a higher tolerance,\n",
    "probably due to accumulation of floating point errors with the additional\n",
    "standardization and destandardization.\n",
    "\"\"\"\n",
    "\n",
    "baseline = jnp.zeros(100)\n",
    "ig = integrated_gradients_all_outputs(model, 10, 24, real_world_ys[:10], baseline[:10], 2000, True)\n",
    "\n",
    "\n",
    "ys_std, mean, std = train_utils.standardize(real_world_ys[:10])\n",
    "\n",
    "baseline = (baseline - mean)/std\n",
    "\n",
    "\n",
    "f = model(ts, ys=ys_std[:10], control_until=10, saveat=ts, train_until=24)\n",
    "f = train_utils.inverse_log_transform(train_utils.destandardize(f, mean, std))\n",
    "\n",
    "f_bar = model(ts, ys=baseline[:10], control_until=10, saveat=ts, train_until=24)\n",
    "f_bar = train_utils.inverse_log_transform(train_utils.destandardize(f_bar, mean, std))\n",
    "\n",
    "\n",
    "ig_sum = jnp.sum(ig, axis=-1)\n",
    "output_difference = f - f_bar\n",
    "\n",
    "print(ig_sum)\n",
    "print(output_difference)\n",
    "\n",
    "assert(jnp.allclose(output_difference[:10], ig_sum[:10], atol=10e-2, rtol=10e-2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
